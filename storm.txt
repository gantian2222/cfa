1.date_format(date,%Y%m%d) 讲date转换为Str
str_to_date(str,%Y%m%d)
substr(str,1,size)
trim()
length()
round(number,1/-1)
rand()  0-1

2.分组函数聚合函数:sum,avg,count,max,min,
只能用于 where 之前，会自动忽略空值

select count(age) from person where age is null;==>0会过滤掉空的

3.distinct必须放在所有字段前面，如果后面有多个字段则为联合去重

4.
group by是先对where后的结果进行分组再执行聚合函数返回
select name, max(age)as age from person group by name;
只能select出分组的 字段
select name, max(age)as age from person group by name;

5.
having是对分组的数据再进行过滤，比如平均薪水大于20000
select avg(sal) avsal 
from person 
group by job 
having avsal>20000
where是用在分组之前，having是用在分组执行聚合函数之后

selec 
	XXXX
from
	XXXX
where
	XXXX
group by
	XXXX
having
	XXXX
order by
	XXXX
limit 1,2;

多表查询
1）
select * from p,n ==>返回结果是两张表记录乘积。n*m 笛卡尔积
可以通过where 进行过滤
select * from p (inner) join n on p.name=n.name 两边都有才返回，就是内连接
2）
内连接
	select * from p (inner) join n on p.name=n.name 两边都有才返回
外连接
全连接

Paxos算法的主要应就是实现一个分布式的状态机，
State Machine Approach的一个关键点是，分发到每台状态机的每笔用户输入的顺序都必须是一样的，在有一个单点专门接受请求并复制分发时，是可以保证这点的

一个最简单的拒绝策略是先来后到,
那么将永远不会出现两个进程的v值相等的情况，即v永远无法被决定。更正式的说，这个协议不满足活性，活性要求协议总能达成一致

---------------------

为什么要过半
因为两个过半集合一定会有一个相交的点，因为为了迅速达成一致性，我们在准备阶段，acceptor会返回已经接受的最大提案的value，
我们的proposar会将其作为新提案的的value,从而下一批过半机器就达成相同的一致。

聚合函数
Stream.aggregate()==>对每个Batch进行聚合，如果前一个是groupBy(),则是对每个分区进行聚合
stream.persistentAggregate()==>对全部Batch进行聚合，并将结果持久化
聚合器:Aggregator,CombinerAggregator,ReducerAggregator
CombinerAggregator会预处理每一个接收到的tuple，将预处理后的值传入combine函数来计算直到最后一个tuple到达，当所有tuple处理完时，CombinerAggregator会发射zero函数的输出
ReducerAggregator通过init方法提供一个初始值，然后为每个输入的tuple迭代这个值，最后生产处一个唯一的tuple输出这个值
函数stateQuery可以在state里面进行查询，查询类extends BaseQueryFunction
更新State:extends BaseStateUpdater,通过partitionPersist函数进行更新调用
Trident有另外一种更新State的方法叫做persistentAggregate，persistentAggregate是在partitionPersist之上的另外一层抽象, 它知道怎么去使用一个Trident 聚合器来更新State
public class TransactionalMap<T> implements MapState<T>   外层函数调用的是该函数的multiGet,multiPut,multiUpdate,其中multiGet是从持久化中获取到取出TransactionValue,去掉tx,只返回
Value,multiPut是将Value包装成TransactionalValue,加上tx调用用户自定义的IBackingMap接口实现类的multiPut进行持久化。
IBackingMap接口实现类：
multiGet：用户自定义持久化，持久化的是TransactionalValue，对我们的value添加了tx
multiPut:用户自定义查询，查询出来的是TransactionalValue，对我们的value添加了tx
TransactionalMap
multiGet：用户使用的，存Value,该方法会构造一个TransactionalValue，替value添加了tx，然后调用IBackingMap接口实现类的multiGet持久化TransactionalValue
multiPut:用户使用的查询，该方法会调用IBackingMap接口实现类的multiPut查询出来TransactionalValue，然后取消tx返回给用户value
multiUpdate：每次调用persistantAggregate时会调用该方法，先进行multiGet，判断是否有，有的话再判断tx，最后再调用聚合器得到输出值通过multiPut持久化到数据库或内存


TreeMap<Long, TransactionStatus> _activeTx = new TreeMap<Long, TransactionStatus>();
TreeMap<Long, Integer> _attemptIds;

private SpoutOutputCollector _collector;
Long _currTransaction;
int _maxTransactionActive;



private static class TransactionStatus {
        TransactionAttempt attempt;
        AttemptStatus status;
		}
		
		
private static enum AttemptStatus {
        PROCESSING,
        PROCESSED,
        COMMITTING
    }
	
	
Ack bolt,接受三个流，init-stream,ack-stream,fail-stream,
通过MessageId生成RootId,每个task也会为其生成一个消息Id
spout发送init-stream，rootId和ackValue
为了减少与Acker Bolt通信的次数，此处进行了 优化。标记消息的跟踪值用来表示在该Bolt上由标记消息所衍生出来的消息的异或值。
例 如，输入消息乃产生T2、T3f(lT4,则输人消息T,的跟踪值为T2ATVT4,在对消息乃进行Ack 操作时，发送给Acker Bolt的值为<RootId,T2AT3AT4>

// 老数据
{
    transactionId：3
    urlId:99
    prevReach:2
    reach:3
}

// 新到数据
{
    //注意事务ID为3，和老数据中的事务ID相同
    transactionId:3
    urlId:99
    reach:5
}
非透明事务，多保存一个前一个结果
因为当相同txid来的时候，没法判断，则舍弃数据库已更新的，采取现在这个结果累加到上一个结果上
// 更新后数据
{
    transactionId:3
    urlId:99
    //这个值不变
    prevReach:2
    //2 + 5 = 7
    reach:7
}
我们应该这么做，考虑到新到数据的事务ID和存储中的事务ID一致，所以这批数据可能被分别或者异步处理了，但是，这批数据对应的事务ID永远是同一个，那么，即使这批数据中的A部分先处理了，由于大家都是一个事务ID，那么A部分的前值是可靠的。
所以，我们将依靠prevReach而不是Reach的值来更新

numPartitions方法会在ITransactionalSpout.initializeTransaction方法中调用，返回每 一个事务所包含的分区数目
isReady方法与ITransactionalSpout.isReady方法的含义相同，表7K数据是否已经准备好 以便开始一个新的事务
emitPartitionBatchNew方法以及emitPartitionBatch方法会在ITransactionalSpout.emit Batch中被调用。
emitPartitionBatchNew方法会在一个事务首次进行尝试时被调用，它会 基于当前分区以及事务，计算获得该分区所对应的元数据
emitPartitionBatch会在重传时被调用。当然，在具体的实现中，emitPartitionBatchNew 方法可以基于emitPartitionBatch方法来实现。区别在于，
emitPartitionBatch方法知道 该分区在当前事务上面的元数据是什么，而emitPartitionBatchNew则需要通过计算来得 到该元数据
for(int i=_index;i < partitions;i+=_numTasks)
_numTasks是并行度2，_index是消息发送节点的索引
则Emiter0,Emiter0会发送0，2，4，6，8，Emiter1会发送1，3，5，7，9分区

第4行设置Topology中的协调Spout节点。它利用TransactionalSpoutCoordinator对 ITransationalSpout中的协调Spout接口进行封装执行。
根据之前的分析可以知道，TransactionalSpoutCoordinator实现了IRichSpout接口。 
□ 第10-15行用于设置Topology中的消息发送节点。首先，利用TransactionalSpoutBatchExecutor对ITransactionalSpout中的消息发送接口进行封装，
然后，再利用CoordinatedBolt进行进一步的封装。在事务Topology中执行的所有Bolt均为CoordinatedBolt类型



prepare,init,aggregate,complete
_agg.aggregate(processorContext.state[_context.getStateIndex()], _projection.create(tuple), _collector);

ChainedAggregatorImpl->有个Aggregate[]
ChainedAggregatorImpl中的中间结果
public class ChainedResult {
    Object[] objs;==》result数组
    TridentCollector[] collectors;==>Aggregator aggregator方法的TridentCollector参数
    }
	
CombinerAggregatorCombineImpl->有个用户传入的Aggregate	
CombinerAggregatorCombineImpl中的中间结果	
public class Result {
    public Object obj;==》自定义的aggregator的中间结果
}
自定义的Aggregator的中间结果
int
所以在执行器上下文中存有ChainedResult,在ChaiedResult中存有每个aggregator执行的结果以及中间执行结果
从上到下调用aggregate,中间结果从下到上传递存放到执行器上下文中

GroupedAggregator的中间结果是一个Object[] arr
GroupCollector groupColl = (GroupCollector) arr[0];==>Aggregator aggregator方法的TridentCollector参数
Map<List, Object> val = (Map) arr[1];==>这个Map就是管理根据分组域创建的TridentTuple作为key,value为各个分组对应的中间结果

TridentBoltExecutor implements IRichBolt是真正的bolt，用其来适配SubtopologyBolt

SubtopologyBolt
Set<Node> _nodes;
在TridentToplogyBuilder中setBolt是假set
BoltDeclarer d = builder.setBolt(boltIds.get(g), new SubtopologyBolt(graph, g.nodes, batchGroupMap), p,committerBatches(g, batchGroupMap), streamToGroup);
TridentToplogyBuilder的buildTopology才真正用topologyBuilder来setBoltsetSpout，
BoltDeclarer bd = builder.setBolt(id,new TridentBoltExecutor(
                          new TridentSpoutExecutor(
                            c.commitStateId,
                            c.streamName,
                            ((ITridentSpout) c.spout)),
                            batchIdsForSpouts,
                            specs),
                        c.parallelism);
						
DirectedGraph _graph;
    Set<Node> _nodes;
    Map<String, InitialReceiver> _roots = new HashMap();==》流和List<TridentProcessor>的映射，当来了一个tuple时，通过流找到对应的所有执行器对其进行处理，
	通过循环节点，找到每个节点的父节点将这个映射关系加到这个map中
    Map<Node, Factory> _outputFactories = new HashMap();
    Map<String, List<TridentProcessor>> _myTopologicallyOrdered = new HashMap();==》key是batchGroup 有序的执行器在调用finishBatch，initBatchState，cleanup方法时会依次调用每个执行器的对应方法
    Map<Node, String> _batchGroups;
	
	
	IdentityGrouping类用来表示在源端组件的Task数目与接收端组件的Task数目相同的情况 下，源端的Task与目的端的Task可以一一对应起来，
	这与直接分组是非常类似的。 在Topology的执行优化过程中，如果两个相邻的节点组通过IdentityGrouping的方式进行了 连接，则表示它们具有相同的并行度，
	于是会将这两个节点组合并。该类的分析如下
	
	它用来计算何时可以对两个节点组进行合并。 其基本思想为：如果一个节点组只有一个父节点组，那么将该节点组与其父节点组进行合并；
	类 似地，如果一个节点组只有一个子节点组，那么将子节点组与自身节点组进行合并；反复进行这 两个过程直到没有满足以上条件的节点组为止
	
	_colocate 变量是一个哈 希表，其键为Stateld, 值为该State上进行操作的节点（即对State进行更新和查询的节点 )。 
	Trident会将这两类节点放在一个节点组中同时处理，以便于State的查询与更新
	
流的分区操作是非常关键的，它对应于Storm中的消息分组方式。在Trident中，消息的分组
方式是通过添加分区节点来完成的。与处理节点不同，分区节点只用来反映处理节点之间的消息接收方式，并不对应于实际的Spout或者Bolt节点

分组流上的聚集操作：
首 先 按 对 输 入 流 进 行 重 新 分 区，然后在每个分区上调 用基于分组的聚集算法。
由于具有相同_groupedFields值的消息都已经在同一个节点上 了，所以返回的将是流对象而不是分组流对象


IPartitionedTransactionalSpout的Coordinator的方法是numPartitions，isReady，close
PartitionedTransactionalSpoutExecutor的Coordinator会实现ITransactionalSpout的Coordinator，对应initializeTransaction，isReady，close方法，其中会调用IPartitionedTransactionalSpout Coordinator对应的方法
Emitter会实现ITransactionalSpout的Emitter<Integer>，在emitBatch中会循环调用IPartitionedTransactionalSpout.Emitter的emitBatch来发送多个分区的内容
Emitter有个成员变量Map<Integer, RotatingTransactionalState> _partitionStates = new HashMap<Integer, RotatingTransactionalState>();存放每个分区的所有发送的tuple的meta信息
RotatingTransactionalState里有个private TreeMap<BigInteger, Object> _curr = new TreeMap<BigInteger, Object>();就是存放的分区里的所有tuple对应的Meta信息
分区执行器PartitionedTransactionalSpoutExecutor会对IPartitionedTransactionalSpout进行适配，包装其Coordinator
public class PartitionedTransactionalSpoutExecutor implements ITransactionalSpout<Integer> {
IPartitionedTransactionalSpout _spout;
Emitter的emitBatch方法
 _index = context.getThisTaskIndex();index是对应task的编号
 for(int i=_index; i < partitions; i+=_numTasks) { 如果task编号是0，有2个task，则该bolt发送0,2,4,6,8分区
 _emitter.emitPartitionBatch(tx, collector, partition, meta);
 }

 
 ----------------------------------------------------------------------------------------------
 通过适配器原理
Trident的Spout都会对我们传入的spout进行适配
ITridentSpout
IPartitionedTridentSpout
IOpaquePartitionedTridentSpout
这三个Spoout是给我自定义的spout实现的，对于他们底层会用PartitionedTransactionalSpoutExecutor和OpaquePartitionedTransactionalSpoutExecutor进行适配再给底层代码调用
其中ITridentSpout不需要适配，同时spout内部的Coordinator和Emitter也需要适配。
适配是为了实现通一的方法，在方法实现中会调用我们自定义的方法，这样底层就相当于调用了我们的方法了，因此适配器PartitionedTransactionalSpoutExecutor和OpaquePartitionedTransactionalSpoutExecutor
都实现了ITridentSpout接口。
内部的Coordinator和Emitter也实现了ITridentSpout里面的Coordinator和Emitter。所以适配器的东西都实现了ITridentSpout，这样就可以当做ITridentSpout使用了
Coordinator是initializeTransaction方法
Emitter是emitBatch方法
在分区事务适配器中PartitionedTridentSpoutExecutor
Coordinator的initializeTransaction方法主要是调用用户自定义的
getPartitionsForBatch获取每个分区的元数据

只在事务第一次的时候调用emitPartitionBatchNew，为每个分区发送数据
当重发时调用emitPartitionBatch，传入Metadata进行重传
_emitter.getOrderedPartitions会获得分区列表，且用户要为每个分区分配一个Id。Emitter的executBatch就会每次去获取分区列表从而可以动态的增加或减少分区。最后用个for循环发送对应的分区
Emitter为每个分区维护了一个EmitterPartitionState状态，key为分区号->分区状态，状态中会保持每个分区的Metadata
_partitionStates.put(id, new EmitterPartitionState(new RotatingTransactionalState(_state, id), p));其中RotatingTransactionalState会帮助用户在ZK上创建一个分区节点，节点名为分区号
将每个分区的Metadata写入对应分区节点里，Metadata由emitPartitionBatchNew产生
RotatingTransactionalState是为每个分区服务的，缓存了每个事务对应的分区状态，所以内部有个Map:txid->分区的Metadata,这个是为重传以及获取上一次的状态作为参数传入emitPartitionBatchNew
而Emitter是为每个Batch服务的，所以内部缓存了当前事务所有分区对应的分区状态，即内部有个Map:分区Id->EmitterPartitionState(RotatingTransactionalState)
Map<String, EmitterPartitionState> _partitionStates = new HashMap<String, EmitterPartitionState>();就是方便下次重传时直接使用而不用在次获取分区状态
循环这个Map,就可以得到每个分区，再根据现在所处的事务，就可以得到在分区中该事务需要的Metadata,即分区Id->EmitterPartitionState->RotatingTransactionalState->再通过txid->分区的Metadata

Emitter仓库管理员管理了当前的任务号，以及仓库和其联络员的Map,当我们的任务(事务)第一次来的时候，会先看手中上一个存档的单号_savedCoordinatorMeta和这次传入的是否是同一个，如果是就说明是重发，
如果不是就说明该任务(事务)是第一次，那么先去问问甘小姐，得到仓库列表(调用Emitter的getOrderedPartitions(coordinatorMeta)分区列表)主要是仓库号(分区Id)列表，
再为每个仓库(分区)分配一个zk联络员，最后会挨个询问联络员他管的仓库的metadata,如果是第一次该任务第一次来，就不会有其对应的Metadata,先会在记录里找到上一个任务对应的Meta,作为参数传入甘小姐Emitter的emitPartitionBatchNew生成Metadata。
并发送出去(_emitter.emitPartitionBatchNew(tx, collector, partition, lastState)),并将metadata记录下来，再将每个仓库和联络员，注册到仓库管理员那里。
如果直接问就有，说明是任务重发，就直接派发(_emitter.emitPartitionBatch(tx, collector, partition, meta))。
每次任务都会解雇掉所有的联络员，所以创建联络员的时候会先去zk那里同步老联络员的小本本，每次都创建是因为怕甘小姐家的仓库变多了或减少了会动态变化，
这也方便了如果仓库管理员消失了，找不到联络员，因为我们直接新招聘。
-spout/partition1/tx1
-spout/partition1/tx2
-spout/partition1/tx3
-spout/partition1/tx4
每次是事务重发，所以以tx为子节点


 MasterBatchCoordinator extends BaseRichSpout
 PartitionNode有个Grouping
Grouping x = new Grouping();
x.set_fields(value);

且分组流上进行的操作都是以分组的列作为聚合单位的。 分组流是多流操作的核心，最终会通过添加分区节点的方式转换成为流。
在Trident底层，将 通过域分组方式（Fields Grouping ) 完成，域分组所采用的域即为分组流的域，这样具有相同分 组域的消息就可以到达相同的节

分组流的partitionAggregate操作是利用GroupedAggregator来完成的。该聚集器会首先调用_ gr0UpFieldS对输人消息进行分组，然后在分组后的集合上调用聚集器agg
先分区再调用

发射器内部都保存了单个处理器的上下文(TridentContext)和所有处理节点的上下文(ProcessorContext)
TridentContext _triContext;
ProcessorContext context;
FreshCollector的emit会直接读取tupleReceiver，调用其execute方法
CaptureCollector的只是将消息放入缓存
GroupCollector 发送分组域和要发的value组成的list
AppendCollector 将输出列加到输入列，再直接调用tupleReceiver的execute方法
AddldCollector 只是为我们发射的tuple添加txid第一列

处理器：协调事务和bolt的各个时期，相应调用下层的方法，为其传入在上层缓存的结果，创建Tuple,缓存tuple
□ TupleReceiver：对基本的消息处理进行抽象。 
□ TridentProcessor：对基本的事务消息处理进行抽象 
□ EachProcessor：会对消息进行逐条处理的处理节点。
□ ProjectedProcessor: 对映射消息进行操作的处理节点。
□ PartitionPersistProcessor: 分区存储的处理节点，它会将数据存储于State对象中。
□ StateOueryProcessor: State查询处理节点，主要用于DRPC。 □ MultiReducerProcessor: 处理多流情况的TridentProcessor
TridentProcessor有事务周期和bolt周期方法
开始事务时调用startBatch
收到消息时调用execute
结束事务时调用finishBatch
bolt创建时调用prepare
bolt销毁时调用cleanup

PartitionPersistProcessor:
FreshCollector,emit会直接调用tupleReceiver的execute方法
ProjectionFactory
State对象存在从Topology Context上下文对象中，通过_stateId得到。初始化SubTopologyBolt时，对state对象进行了集中的创建， 
在startBatch中对Topology Context上下文的状态对象(processorContext.state[_context.getStateIndex())进行实例化，是一个list= new ArrayList<TridentTuple>() 
在execute中为每个输入生成输入Tuple缓存在Topology Context上下文的的状态对象中(processorContext.state[_context.getStateIndex())
在finishBatch中才对这些tuple进行处理，开启commit,进行更新，commit,批处理思想，在finishBatch才进行统一更新
			_state.beginCommit(txid);
            _updater.updateState(_state, buffer, _collector);
            _state.commit(txid);
			
StateOueryProcessor和PartitionPersistProcessor相对，两个将放在一个bolt内执行
AppendCollector 将输出列加到输入列，再直接调用tupleReceiver的execute方法
同理在finishBatch中调用OueryFunctionfunction的execute方法

AggregateProcessor
FreshCollector 
startBatch中调用Aggregator的init方法,并将结果存在processorContext.state[_context.getStateIndex()]中
execute中调用Aggregator的aggregate方法,中间值从processorContext.state[_context.getStateIndex()]中取回。需要传入colletor，因为可能需要把消息发出去
finishBatch中调用Aggregator的complete方法，需要传入colletor，因为可能需要把消息发出去
例如其CombinerAggregatorCombineImpl(是对用户的CombinerAggregator适配器)的init的结果是个Result对象，所以这个对象就保存在processorContext.state[_context.getStateIndex()]中。
Result.value就是保存我们自己的value


AggregateProcessor->内部是GroupedAggregator->再GroupedAggregator的内部是CombinerAggregatorCombineImpl
当调用startBatch->调用GroupedAggregator的init,返回new Object[] {new GroupCollector(collector, _fact), new HashMap(), batchId}，这个会缓存在processorContext.state[_context.getStateIndex()]中
当调用execute,拿到缓存传入->调用GroupedAggregator的aggregate->从processorContext.state[_context.getStateIndex()]中拿到GroupCollector和该tuple所属组对应的中间结果传入->CombinerAggregatorCombineImpl的aggregate，这里不会emit
当调用finishBatch,拿到缓存传入->GroupedAggregator的complete->为每个分组调用_agg.complete(分组对应的中间值, groupColl);在整个batch处理完毕才会emit

EachProcessor
AppendCollector
startBatch空
execute：为每个tuple调用_function.execute
finishBatch空

ProjectedProcessor
没有Collector
startBatch空
execute：挨个执行处理器的execute
		for(TupleReceiver r: _context.getReceivers()) {
            r.execute(processorContext, _context.getOutStreamId(), toEmit);
        }
finishBatch空
在processor中构建的tridentTuple都是TridentTupleView，
内含ValuePointer[] _index;-->通过index找到索引指针
    Map<String, ValuePointer> _fieldIndex;->通过field名找到索引指针
    IPersistentVector _delegates;-->通过索引指针中的层索引和index在该List<List<Object>>中找到value

	
ProcessorContext：保存了每个处理器的缓存结果，实现了数据在各处理器间的共享。创建的一个对象，传入该bolt上所有的TridentProcessor的startBatch
public Object batchld; 
public Object[] state;

TridentContext单个处理节点的上下文
□ selfFields为该处理节点新产生的列。 
□ parentFactories和parentStreams分别为该节点的父节点的消息模式和消息对应的流。 
口 receivers表明哪些处理节点将接收该节点产生的消息。 
□ outStreamld为该节点的输出消息流
□ statelndex为该处理节点的编号，为SubTopologyBolt中的统一编号， 
□ collector用来向外发送消息

10个分区
task1:0,2,4,6,8,10
task2:1,3,5,7,9